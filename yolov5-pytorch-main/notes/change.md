7月22日

# 模型改动

### 注意

- **注意参数之间的对应关系，以及特征图的shape匹配问题**

### 改动思路：

- 尽可能少的代码，得到高效的结果
- 根据评价指标，看看哪里可以欠缺（补），那个指标影响不大（省），哪里可以更好（改）
- 由任务结果思考评价指标，可能有更合适的指标，是否能用到网络里，是否能根据这个指标是检测效果接近真实结果
- 网络的深度和广度是网络提取信息的重点

### 参考方向

以下是考虑改动的地方

1. 激活函数    关于求导

2. 损失函数    关于更新

3. 学习率lr      关于更新速度

4. 优化器        优化loss lr 

5. 数据增强     关于数据预处理

6. 添加模块     如注意力集中机制

7. 网络修改     注意结构合理融合

8. 新奇可用的小想法

   ......

   有余力加上deep sort



### 相关概念

#### 正则化

- Regularization，中文翻译过来可以称为**正则化**，或者是**规范化**。什么是规则？闭卷考试中不能查书，这就是规则，一个**限制**。同理，在这里，规则化就是说给**损失函数**加上一些限制，通过这种规则去规范他们再接下来的循环迭代中，不要自我膨胀。

#### 梯度弥散

- 由于导数的链式法则，连续多层小于1的梯度相乘会使梯度越来越小，最终导致某层梯度为0。

#### 梯度爆炸

- 由于导数的链式法则，连续多层大于1的梯度相乘会使梯度越来越大，最终导致梯度太大的问题



## 激活函数

- 注：这里的处理对象 x 是t ensor

#### SiLU

y = x*torch.sigmoid（x）

原主使用的是SiLU函数，也叫做swish函数，其图像如下：

![image-20220723130827426](C:\Users\Happy\AppData\Roaming\Typora\typora-user-images\image-20220723130827426.png)

SiLU激活函数的优点，无界性方面有利于防止慢速训练期间，梯度接近于0并导致饱和；导数恒大于0；平滑度在优化和泛化中起很大作用。SiLU 的激活大约等于ReLU的激活，SiLU 的一个很强特点是它具有自稳定特性。导数为零的全局最小值在权重上起到“软地板”的作用，作为隐式正则化器，抑制了大数量权重的学习。



#### 以下是常见的几个激活函数



![image-20220723130843685](C:\Users\Happy\AppData\Roaming\Typora\typora-user-images\image-20220723130843685.png)







#### hardswish_H_SiLU

![image-20220805084055473](C:\Users\Happy\AppData\Roaming\Typora\typora-user-images\image-20220805084055473.png)



![image-20220725145821689](C:\Users\Happy\AppData\Roaming\Typora\typora-user-images\image-20220725145821689.png)

因为swish中sigmoid的计算程度太高，对硬件配置要求高，就会在大模型部署上的运算速度，于是就出现了H_SiLU分段函数，在江都计算强调的同时尽可能的等效于swish函数。在相关实验中，发现所有这些功能的硬版本在精度上没有明显的差异，但从部署的角度来看，有多种优势。首先，在几乎所有的软件和硬件框架上都可以使用经过优化的ReLU6实现。其次，在量子化模式下，它消除了由于近似sigmoid的不同实现而造成的潜在数值精度损失，分段函数可以减少内存访问的数量，从而大幅降低延迟成本。



#### sigmoid（）

y = 1/(1 +torch.exp(-x) )

y' = y( 1 - y )

- 好：sigmoid可以对输出归一化，范围0到1；很适合以预测概率为输出的模型，比如二分类；（有界性也是有优势的，因为有界激活函数可以具有很强的正则化，并且较大的负输入问题也能解决）；梯度平滑，避免跳跃出值。
- 不好：函数输出并非以0为中心，权重更新较慢；指数运算，计算机运行较慢；容易梯度弥散，当 y 趋向于 1 和-1 时称为过饱和，处于饱和状态的激活函数意味着当 100 x = 和 1000 x = 时的反映是一样的，这样的特征转换会造成信息给丢失，Sigmoid 函数在 x 取值在-3 到 3 之间应该会有比较好的效果。



#### tanh（）

- tanh函数也是双切函数的一种，他补充了sigmoid函数不适是以0为中心输出的问题，然而，梯度消失的问题和幂运算问题仍旧存在。



#### relu（）

torch

- 好：线性整流函数，正线性单元relu，是网络模型中常用的激活函数；输入为非零时，不存在梯度饱和问题；计算速度快，模型收敛快，模型训练时间较短，且很少出现梯度弥散现象。
- 不好：输出不是以0为中心；在训练过程中，很可能会出现大量的神经元死亡失效，可以通过减小学习率来解决或其变体函数。



#### leaky relu（）

- 为解决dead relu的问题而设计的激活函数，通过一个a值来扩大relu的范围，在yolov3中发挥很好的效果。



#### elu（）

- 好：relu的另一个变体，elu有负值，通过减少偏置偏移的影响，使正常梯度更接近于单位自然梯度，从而使均值向零加速学习；同样没有dead relu问题，输出的平均值接近于零；函数在较小的输入下会饱和至负值，从而减少前向传播的变异和信息；
- 不好：计算强度高，实践中也不总是比relu好，应用不广泛。



#### prelu（）

- 大于0部分为relu，小于等于0部分为leaky relu，即取两者优点。



#### FReLU

-  https://arxiv.org/pdf/2007.11824.pdf       ECCV2020 | FReLU：旷视提出一种新的激活函数，实现像素级空间信息建模

- Funnel激活函数 (FReLU)，通过增加一个空间条件 (见图2)来扩展ReLU/PReLU函数，它是结合卷积实现的带参数的激活函数，增加了一个可以忽略不计的计算开销。 该激活函数的形式是**y=max (x,T (x))，**其中T (x)代表简单高效的空间上下文特征提取器。 由于使用了空间条件，FReLU简单地将ReLU和PReLU扩展为具有像素化建模能力的视觉参数化ReLU。比以前的激活函数更有效,实验环节中，在分类网络中替换了正常的ReLU.
- 空间条件spatial condition以简单的方式实现了像素级建模能力，并通过常规卷积捕获了复杂的视觉layouts。



#### 其他激活函数

- maxout 函数
- softplus 函数
- softsign 函数
- 高斯误差线性单元 GELUs



## 注意力集中机制

注意力集中机制已经写到attention_block.md文件里了。学会了4个比较简单的，也看了多头注意力集中机制，还有一些 transformer 的讲解，但因为对transformer理解不深，且transformer参数大，目前把学会的attention添加到网络里进行测试，简单做了个消融实验。



## 损失函数







## 网络修改

1. 增加网络的深度，但又要保持网络不过于复杂，保持网络轻量化，drop机制，为了尽量使广度的减少量更小，用1*1的filter

   
